from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# Load the tokenizer and model for Q/A
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-distilled-squad")
model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased-distilled-squad")

while True:
    # Read a question from standard input
    question = input("Question: ")

    # Tokenize input
    context = "OpenAI created GPT-3, a state-of-the-art language model."
    inputs = tokenizer.encode_plus(question, context, return_tensors="pt")

    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)
        answer_start = torch.argmax(outputs.start_logits)
        answer_end = torch.argmax(outputs.end_logits) + 1
        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))

    # Print the answer
    print("Answer:", answer)
